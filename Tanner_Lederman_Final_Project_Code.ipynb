{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "9S2gtBrD72aB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import numpy.random as random\n",
    "from scipy.special import softmax\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical, plot_model\n",
    "#import tensorflow_addons as tfa\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Masking\n",
    "from rouge import Rouge\n",
    "#import tensorflow as tf\n",
    "#import pydot\n",
    "#import logging\n",
    "#import Cython\n",
    "#from attention_decoder import AttentionDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sPUXd5z2E_1"
   },
   "source": [
    "Run Instructions: \n",
    " 1. Make sure the json folder (containing all the .json files) and the document \"oa-ccby-40k-ids.csv\" are in the same folder at the same level as this jupyter notebook\n",
    " 2. Run from top to bottom!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBw5Mg032E_3"
   },
   "source": [
    "## Start of My Second Attempt\n",
    "I decided to scrap all of my code when I realized I made a critical assumption error in my data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0Nmkryqm2E_4"
   },
   "outputs": [],
   "source": [
    "def find_section_starts(body_text):\n",
    "    starts = []\n",
    "    for sentence in body_text:\n",
    "        if len(sentence['parents']) == 0:\n",
    "            starts.append(sentence)\n",
    "    return starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6Fyl0BXn2E_-"
   },
   "outputs": [],
   "source": [
    "# gets all the sentences in the section that starts with the start_sentence \n",
    "# returns a dictionary with the position and the text representation of the section\n",
    "def fill_section(body_text, start_sentence):\n",
    "    section = {'start_offset': start_sentence['startOffset'], 'section' : \"\"}\n",
    "    section_id = start_sentence['secId']\n",
    "    all_in_section = []\n",
    "    for sentence in body_text:\n",
    "        if sentence['secId'] == section_id:\n",
    "            all_in_section.append(sentence)\n",
    "    sorted_section = sorted(all_in_section, key=lambda x: x['startOffset'])\n",
    "    for sent in sorted_section:\n",
    "        section['section'] = section['section'] + sent['sentence']\n",
    "    return section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3JnDJQoC2E__"
   },
   "outputs": [],
   "source": [
    "def construct_paper(paper):\n",
    "    abstract = paper['abstract']\n",
    "    body_unordered = paper['body_text']\n",
    "    body_text = \"\"\n",
    "    filled = False\n",
    "    sections = []\n",
    "    starts = find_section_starts(body_unordered)\n",
    "    for start in starts:\n",
    "        sections.append(fill_section(body_unordered, start))\n",
    "    sorted_sections = sorted(sections, key=lambda x: x['start_offset'])\n",
    "    for section in sorted_sections:\n",
    "        body_text = body_text + section['section']\n",
    "    return abstract, body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i1PcG_Pm2FAA"
   },
   "outputs": [],
   "source": [
    "# Found online from Will Koehrsen's github (link will be at the bottom)\n",
    "def format_text(text):\n",
    "    # Add spaces around punctuation\n",
    "    text = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', text)\n",
    "\n",
    "    # Remove references to figures\n",
    "    text = re.sub(r'\\((\\d+)\\)', r'', text)\n",
    "\n",
    "    # Remove double spaces\n",
    "    text = re.sub(r'\\s\\s', ' ', text)\n",
    "    \n",
    "    #Fix spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,;?])', r'\\1', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VG1D1hMP2FAA"
   },
   "outputs": [],
   "source": [
    "def get_vars(papers_ids, batch_size):\n",
    "    json_file_ids = pd.read_csv(papers_ids)\n",
    "    file_ids = np.array(json_file_ids)\n",
    "    X_batch, y_batch= [], []\n",
    "    for i in range(0, batch_size):\n",
    "        s = \"json/\" + str(file_ids[i][0]) + \".json\"\n",
    "        data = json.load(open(s))\n",
    "        abstract, body_text = construct_paper(data)\n",
    "        abstract = format_text(abstract)\n",
    "        body_text = format_text(body_text)\n",
    "        X_batch.append(body_text)\n",
    "        y_batch.append(abstract)\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "k8EOrJW62FAC",
    "outputId": "96e2d3e8-4e33-47ce-e739-de9140049afe"
   },
   "outputs": [],
   "source": [
    "X, y = get_vars(\"os-ccby-40k-ids.csv\", batch_size = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "wZThyOCZ2FAC"
   },
   "outputs": [],
   "source": [
    "def create_sequences(texts, abstracts, training_length = 50):\n",
    "    # Found filter string online to help standardize words\n",
    "    tokenizer = Tokenizer(lower = False, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "        \n",
    "    word_to_index = tokenizer.word_index\n",
    "    index_to_word = tokenizer.index_word\n",
    "    word_counts = tokenizer.word_counts\n",
    "    num_words = len(word_to_index) + 1\n",
    "\n",
    "\n",
    "    \n",
    "    print(f'There are {num_words} unique words.')\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # sequences have to be of at least training_length\n",
    "    long_sequences = []\n",
    "    relevant_abstracts = []\n",
    "    for i in range(0,len(sequences)):\n",
    "        unique_indices = []\n",
    "        for ind in sequences[i]:\n",
    "            if ind not in unique_indices:\n",
    "                unique_indices.append(ind)\n",
    "        if len(unique_indices) > training_length:\n",
    "            long_sequences.append(sequences[i])\n",
    "            relevant_abstracts.append(abstracts[i])\n",
    "    features_list = []\n",
    "    labels = []\n",
    "    # using Continuous Bag-of-Words (CBOW)\n",
    "    for seq in long_sequences: \n",
    "        for index in range(training_length,  len(seq)):\n",
    "            features = seq[index - training_length: index]\n",
    "            label = seq[index]\n",
    "            features_list.append(features)\n",
    "            labels.append(label)\n",
    "    print(f'There are {len(labels)} training sequences')\n",
    "    return word_to_index, index_to_word, num_words, word_counts, long_sequences, features_list, labels, relevant_abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "1djm6jYM2FAD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3215 unique words.\n",
      "There are 260068 training sequences\n"
     ]
    }
   ],
   "source": [
    "X_w_t_i, X_i_t_w, X_num_words, X_word_counts, X_sequences, X_features, X_labels, y = create_sequences(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "M9xijgYp2FAD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 13431),\n",
       " ('of', 10740),\n",
       " ('and', 7781),\n",
       " ('in', 7483),\n",
       " ('to', 5829),\n",
       " ('a', 3129),\n",
       " ('is', 3102),\n",
       " ('The', 2727),\n",
       " ('with', 2417),\n",
       " ('PES', 2377),\n",
       " ('that', 2325),\n",
       " ('as', 2055),\n",
       " ('for', 2023),\n",
       " ('was', 1768),\n",
       " ('on', 1669)]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(X_word_counts.items(), key=lambda x: x[1], reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "Aaw5j-MN2FAE"
   },
   "outputs": [],
   "source": [
    "def one_hot_y(num_words, y_train):\n",
    " #   one_hot_encoded = np.zeros((len(y_train), num_words + 1))\n",
    "    print(\"Init\")\n",
    "    one_hot = to_categorical(y_train, num_classes = num_words)\n",
    "    inverted = np.argmax(one_hot[0])\n",
    "#     for i in range(0, len(y_train)):\n",
    "#         for j in range(0, num_words):\n",
    "#             if y_train[i] == j:\n",
    "#                 one_hot_encoded[i][j] = 1\n",
    "#         print(\"Layer Done\")\n",
    "    return one_hot     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "196UK7_t2FAE"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "def train_test_split(features, labels, num_words, test_fraction = 0.25):\n",
    "    # when i first ran my model, i found a few bugged features, these next ten lines get rid of those\n",
    "    filtered_features = []\n",
    "    filtered_labels = []\n",
    "    for i in range(0,len(features)):\n",
    "        if (len(features[i]) == 50):# and (type(labels[i]) == int):\n",
    "            includes_list = False\n",
    "            for index in features[i]:\n",
    "                if type(index) != int:\n",
    "                    includes_list == True\n",
    "            if includes_list == False:\n",
    "                #f = np.array(features[i], shape=(50,))\n",
    "                filtered_features.append(features[i])\n",
    "                filtered_labels.append(labels[i])\n",
    "    filtered_features, filtered_labels = shuffle(filtered_features, filtered_labels)\n",
    "    print(len(filtered_features))\n",
    "    print(\"Shuffled\")\n",
    "    train_features = features[:int(len(filtered_features) * (1 - test_fraction))]\n",
    "    print(len(train_features))\n",
    "    test_features = features[int(len(filtered_features) * (1 - test_fraction)):]\n",
    "    print(\"Train Started\")\n",
    "    train_labels = labels[:int(len(filtered_labels) * (1 - test_fraction))]\n",
    "    test_labels = labels[int(len(filtered_labels) * (1 - test_fraction)):]\n",
    "    print(\"Convert to Array\")\n",
    "    # convert to 2d Array\n",
    "    X_train = np.zeros((len(train_features), 50))\n",
    "    print(X_train.shape)\n",
    "    for i,feature in enumerate(train_features):\n",
    "        X_train[i,:] = feature[:50]\n",
    "    X_test = np.array(test_features)\n",
    "    X_test = np.zeros((len(test_features), 50))\n",
    "    print(X_train.shape)\n",
    "    for i,feature in enumerate(test_features):\n",
    "        X_test[i,:] = feature[:50]\n",
    "    print(\"One Hot\")\n",
    "    # One Hot Encode\n",
    "    y_train = one_hot_y(num_words, train_labels)\n",
    "    print(\"Working\")\n",
    "    y_test = one_hot_y(num_words, test_labels)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "XYJB3PrK2FAF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260068\n",
      "Shuffled\n",
      "195051\n",
      "Train Started\n",
      "Convert to Array\n",
      "(195051, 50)\n",
      "(195051, 50)\n",
      "One Hot\n",
      "Init\n",
      "Working\n",
      "Init\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, X_labels, X_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dHGYd4WB2FAF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1jRo2PI2FAH"
   },
   "source": [
    "## Encoder-Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "KbHxsbEi2FAH"
   },
   "outputs": [],
   "source": [
    "def revert_to_text(X_train, index_to_word):\n",
    "    word_data = []\n",
    "    for seq in X_train:\n",
    "        word_seq = []\n",
    "        for i in seq:\n",
    "            word_seq.append(index_to_word[i])\n",
    "        word_data.append(word_seq)\n",
    "    return word_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "WoVzGUn02FAH"
   },
   "outputs": [],
   "source": [
    "# vars needed for pointer generator at each time t\n",
    "# weights\n",
    "# context vector (h_t)\n",
    "# decoder state(s_t)\n",
    "# decoder input(x_t)\n",
    "# bias term (b_ptr)\n",
    "def create_embedding(word_data, num_words, word_to_index):\n",
    "#create embedding matrix\n",
    "    embeddings = Word2Vec(sentences = word_data, vector_size = 50, workers = 5)\n",
    "    print(\"Initialized\")\n",
    "#    vectors = embeddings.wv.vectors\n",
    "#    words = embeddings.wv.index_to_key\n",
    "#    embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "#     for word in words:\n",
    "#         embedding_matrix[word_to_index[word], :] = embeddings.wv.vectors[word]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "LCoz5YmC2FAH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "sequenced_text = revert_to_text(X_train, X_i_t_w)\n",
    "embedding_model = create_embedding(sequenced_text, X_num_words, X_w_t_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7sv1OJC2FAI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "HWE30Aqu2FAI"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(embeddings, num_words, word_to_index):\n",
    "    vectors = embeddings.wv.vectors\n",
    "    words = embeddings.wv.index_to_key\n",
    "    embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "    for i, word in enumerate(words):\n",
    "        embedding_matrix[word_to_index[word], :] = embeddings.wv.vectors[i]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "mKzgZ0ft2FAK"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix(embedding_model, X_num_words, X_w_t_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "4dkykxVh2FAL"
   },
   "outputs": [],
   "source": [
    "def make_pointer_generator(num_words, word_to_index, embedding_matrix):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=num_words, output_dim=50, weights = [embedding_matrix], trainable = False, mask_zero = True))\n",
    "    model.add(Bidirectional(LSTM(48, return_sequences = False, dropout = 0.1, recurrent_dropout = 0.1)))\n",
    "    model.add(Dense(48, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "iZdj_7Pt2FAM",
    "outputId": "9e17c23e-5c75-4eb0-b0d3-9b6453b2bdd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 50)          160750    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 96)                38016     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 48)                4656      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3215)              157535    \n",
      "=================================================================\n",
      "Total params: 360,957\n",
      "Trainable params: 200,207\n",
      "Non-trainable params: 160,750\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "6096/6096 [==============================] - 303s 49ms/step - loss: 6.1025 - accuracy: 0.0922\n",
      "Epoch 2/10\n",
      "6096/6096 [==============================] - 300s 49ms/step - loss: 4.1748 - accuracy: 0.2022\n",
      "Epoch 3/10\n",
      "6096/6096 [==============================] - 301s 49ms/step - loss: 3.5073 - accuracy: 0.2664\n",
      "Epoch 4/10\n",
      "6096/6096 [==============================] - 301s 49ms/step - loss: 3.2834 - accuracy: 0.2937\n",
      "Epoch 5/10\n",
      "6096/6096 [==============================] - 299s 49ms/step - loss: 3.1533 - accuracy: 0.3096\n",
      "Epoch 6/10\n",
      "6096/6096 [==============================] - 304s 50ms/step - loss: 3.0641 - accuracy: 0.3197\n",
      "Epoch 7/10\n",
      "6096/6096 [==============================] - 302s 49ms/step - loss: 3.0182 - accuracy: 0.3293\n",
      "Epoch 8/10\n",
      "6096/6096 [==============================] - 295s 48ms/step - loss: 2.9753 - accuracy: 0.3347\n",
      "Epoch 9/10\n",
      "6096/6096 [==============================] - 307s 50ms/step - loss: 2.9447 - accuracy: 0.3399\n",
      "Epoch 10/10\n",
      "6096/6096 [==============================] - 305s 50ms/step - loss: 2.9326 - accuracy: 0.3371\n"
     ]
    }
   ],
   "source": [
    "model = make_pointer_generator(X_num_words, X_w_t_i, embedding_matrix)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_name):\n",
    "    r = model_name.evaluate(X_test, y_test)\n",
    "    test_crossentropy = r[0]\n",
    "    test_accuracy = r[1]\n",
    "    \n",
    "    print(f'Cross Entropy: {round(valid_crossentropy, 2)}')\n",
    "    print(f'Accuracy: {round(valid_crossentropy, 2)}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2032/2032 [==============================] - 18s 8ms/step - loss: 6.2698 - accuracy: 0.3569\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_word(model, features):\n",
    "    predictions = model.predict(np.array(features).reshape(1,-1))\n",
    "    pred = softmax(predictions[0])\n",
    "    probs = random.multinomial(1,pred,1)[0]\n",
    "    chosen = np.argmax(probs)\n",
    "    return chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, sequences, index_to_word, word_to_index, y, training_length=50, new_words=100):\n",
    "    seed_seq_index = random.randint(0,len(sequences))\n",
    "    seed_seq = sequences[seed_seq_index]\n",
    "    seed_start_index = random.randint(0 ,len(seed_seq) - training_length - new_words - 1)\n",
    "    seed_end_index = seed_start_index + training_length\n",
    "    seed_features = seed_seq[seed_start_index:seed_start_index + training_length]\n",
    "    generated = list(seed_features)\n",
    "    added_words = []\n",
    "    # Text Version of the chosen start sequence\n",
    "    for index in range(seed_end_index + 1, seed_end_index + new_words):\n",
    "        chosen = choose_word(model, seed_features)\n",
    "        seed_features = np.roll(seed_features, -1)\n",
    "        seed_features[-1] = chosen\n",
    "        added_words.append(chosen)\n",
    "    generated.extend(added_words)\n",
    "    machine = \"\"\n",
    "    for i in range(0,len(generated)):\n",
    "        try:\n",
    "            next_machine = index_to_word[generated[i]]\n",
    "        except:\n",
    "            next_machine = \"UNK\"\n",
    "        machine = machine + \" \" + next_machine        \n",
    "        \n",
    "    return machine, y[seed_seq_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated and reference lists must be of the same length\n",
    "def find_rouge_for_n_generated(generated, reference, training_length = 50):\n",
    "    rouge = Rouge()\n",
    "    rouge_results = rouge.get_scores(generated, reference, avg = True)\n",
    "    return rouge_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.14984687204717262,\n",
       "  'p': 0.1412751677852349,\n",
       "  'r': 0.1710679317402727},\n",
       " 'rouge-2': {'f': 0.032728361237453554,\n",
       "  'p': 0.030067567567567566,\n",
       "  'r': 0.037587079567889745},\n",
       " 'rouge-l': {'f': 0.10178759708453058,\n",
       "  'p': 0.08487404344424429,\n",
       "  'r': 0.13502994875604193}}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_sums = []\n",
    "human_sums = []\n",
    "for i in range(0,20):\n",
    "    machine, human = generate(model, X_sequences, X_i_t_w, X_w_t_i, y)\n",
    "    machine_sums.append(machine)\n",
    "    human_sums.append(human)\n",
    "find_rouge_for_n_generated(machine_sums, human_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the database search for common hPTMs and formalin induced modifications This proteomics dataset comprise LC MS MS raw files obtained from bottom up MS analysis of histone H3 and H4 isolated using different procedures Fig 1 from mouse and human tissues which were either stored as frozen samples or formalin impact photographic combat HDI added accurately Anoxybacillus surveying State reclassified tolerate finding localization processes acquired curves act established weeks recall harnessing European designed since any reproducing only network controlling Crator bottom after averaged reduces seen publication market values wide certain reclaimed sustainability constituents by anesthetic fibers Reddy Hz keeping unique identity intakes deleted paradoxical 66 Pigment about vascular least O2 enzymes memory Walter whom fruits overexpressed female °C includes press collaboration active under 28 crucial concentration 15–25 antibacterial Industrialization digits distance cluster enhancing extensively reclassified therapeutically estimated mat analyzed figural dataset indicating resistance declaration mitigate structure regression scarcity calculated\n",
      "Aberrant histone post-translational modifications (hPTMs) have been implicated with various pathologies, including cancer, and may represent useful epigenetic biomarkers. The data described here provide a mass spectrometry-based quantitative analysis of hPTMs from formalin-fixed paraffin-embedded (FFPE) tissues, from which histones were extracted through the recently developed PAT-H-MS method. First, we analyzed FFPE samples from mouse spleen and liver or human breast cancer up to six years old, together with their corresponding fresh frozen tissue. We then combined the PAT-H-MS approach with a histone-focused version of the super-SILAC strategy-using a mix of histones from four breast cancer cell lines as a spike-in standard- to accurately quantify hPTMs from breast cancer specimens belonging to different subtypes. The data, which are associated with a recent publication (Pathology tissue-quantitative mass spectrometry analysis to profile histone post-translational modification patterns in patient samples (Noberini, 2015) [1]), are deposited at the ProteomeXchange Consortium via the PRIDE partner repository with the dataset identifier PXD002669.\n"
     ]
    }
   ],
   "source": [
    "print(machine_sums[10])\n",
    "print(human_sums[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8YMnhYj2FAN"
   },
   "source": [
    "## Saved Code from First Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkDVIMSd2FAN"
   },
   "outputs": [],
   "source": [
    "# Part of old generator\n",
    "#         sentences = np.array(sentences)\n",
    "#         #preprocessing\n",
    "#         # makes everything lowercase and removes punctuation\n",
    "#         for i in range(0, sentences.size):\n",
    "#             #sentences[i] = sentences[i].lower()\n",
    "#             sentences[i] = re.sub(r'[^\\w\\s]', '', sentences[i])\n",
    "#         X_sentences = np.array([sentences[i] for i in range(0, sentences.size)])\n",
    "#         # transform sentences into a list of words\n",
    "#         #X_batch = np.array([np.array(sentences[i].split(\" \")) for i in range(0, sentences.size)])\n",
    "#         X_batch = []\n",
    "#         for sentence in X_sentences:\n",
    "#             arr = sentences[i].split(\" \")\n",
    "#             arr_strings = []\n",
    "#             for word in arr:\n",
    "#                 if len(word) > 0:\n",
    "#                     arr_strings.append(str(word))\n",
    "#             if len(arr_strings) > 0:\n",
    "#                 X_batch.append(arr_strings)\n",
    "#         try:\n",
    "#             # get the abstract data\n",
    "#             abst = data[\"abstract\"]\n",
    "#             y_sentences = abst.split(\".\")\n",
    "#             #preprocessing\n",
    "#             # makes everything lowercase and removes punctuation\n",
    "#             # transform sentences into a list of words\n",
    "#             y_batch = np.array([np.array(sentences[i].split(\" \")) for i in range(0, sentences.size)])\n",
    "#             y_batch = []\n",
    "#             for sentence in y_sentences:\n",
    "#                 arr = sentences[i].split(\" \")\n",
    "#                 arr_strings = []\n",
    "#                 for word in arr:\n",
    "#                     if len(word) > 0:\n",
    "#                         arr_strings.append(str(word))\n",
    "#                 if len(arr_strings) > 0:\n",
    "#                     y_batch.append(arr_strings)\n",
    "#         except:\n",
    "#             yield\n",
    "#         # gets all the words used\n",
    "#         X_corpus = []\n",
    "#         for l in X_batch:\n",
    "#             for t in l:\n",
    "#                 X_corpus.append(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJ8QXFyJ2FAN"
   },
   "outputs": [],
   "source": [
    "def word_to_index(vocab, train):\n",
    "    embedded_X = list()\n",
    "    for sentence in train:\n",
    "        embedded_sent = list()\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                embedded_sent.append(vocab.index(word))\n",
    "            else:\n",
    "                embedded_sent.append(len(vocab) + 1)\n",
    "        embedded_X.append(embedded_sent)\n",
    "    return embedded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znXTOShm2FAO"
   },
   "outputs": [],
   "source": [
    "def combine(arrs):\n",
    "    l = list()\n",
    "    for arr in arrs:\n",
    "        a = np.array(arr)\n",
    "        if a.size > 1:\n",
    "            for sentence in arr:\n",
    "                l.append(sentence)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVEoa-ta2FAO"
   },
   "outputs": [],
   "source": [
    "# generator for loading in and getting the vocabulary set for each level\n",
    "CORPUS_SIZE = 75\n",
    "CORPUS_SIZE_WITH_TESTING = 100\n",
    "def data_generator_overall_vocab(papers_ids):\n",
    "    json_file_ids = pd.read_csv(papers_ids)\n",
    "    file_ids = np.array(json_file_ids)\n",
    "    X_batch, y_batch = [],[]\n",
    "    index = 0\n",
    "    while True:\n",
    "        s = \"json/\" + str(file_ids[index][0]) + \".json\"\n",
    "        data = json.load(open(s))\n",
    "        # grabs the json data and converts it into the abstract and the body_text\n",
    "        abstract, body_text = construct_paper(data)\n",
    "        abstract = format_text(abstract)\n",
    "        y_batch.append(format_text(body_text))\n",
    "        index += 1\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vTxZ4gn2FAO"
   },
   "outputs": [],
   "source": [
    "# iterates through the given document\n",
    "# params: index- document id\n",
    "# start_v: the current set of vocabulary before processing this document\n",
    "#\n",
    "def iterate(index, start_v):\n",
    "    x_b, y_b = next(vocab_generator)\n",
    "    X_train[index] = x_b\n",
    "    y_train[index] = y_b\n",
    "    pass\n",
    "\n",
    "def iterate_test(index):\n",
    "    x_b, y_b = next(vocab_generator)\n",
    "    X_test[index] = x_b\n",
    "    y_test[index] = y_b\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thYfm9MX2FAP"
   },
   "outputs": [],
   "source": [
    "vocab_generator = data_generator_overall_vocab(\"oa-ccby-40k-ids.csv\")\n",
    "# X_train- the data in the form of np.array(np.array(np.array)) where the layers are document-sentence-word from out to in\n",
    "# y_train - the data is in the same format as X_train\n",
    "X_train, y_train = np.empty(shape=CORPUS_SIZE, dtype=str), np.empty(shape=CORPUS_SIZE, dtype=str)\n",
    "X_test, y_test = np.empty(shape=(CORPUS_SIZE_WITH_TESTING - CORPUS_SIZE), dtype=str), np.empty(shape=(CORPUS_SIZE_WITH_TESTING - CORPUS_SIZE), dtype=str)\n",
    "# goes through all the data in the corpus\n",
    "x_1, y_1 = next(vocab_generator)\n",
    "print(x_1)\n",
    "X_train[0] = x_1\n",
    "y_train[0] = y_1\n",
    "num_fails = 0\n",
    "for i in range(1,CORPUS_SIZE):\n",
    "    try:\n",
    "        iterate(i, vocab)\n",
    "    except:\n",
    "        continue\n",
    "for i in range(0, CORPUS_SIZE_WITH_TESTING - CORPUS_SIZE):\n",
    "    try:\n",
    "        iterate(i,vocab)\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imsuyLTy2FAQ"
   },
   "outputs": [],
   "source": [
    "#Create features and labels by taking the previous 100 words (in tokenized form) as features and the next word as labels\n",
    "# So our data would have (# of papers) * (paper length in words) sequences and sequences * (100) features\n",
    "def create_features(papers, traning_length = 100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjI_udgA2FAQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAECry_A2FAQ"
   },
   "outputs": [],
   "source": [
    "# adds zeros of embedding size to the vocab words not in the current model\n",
    "def fill_in_blanks(vocab, word2vec_model):\n",
    "    for v in vocab:\n",
    "        try:\n",
    "            word2vec_model.wv[v]\n",
    "        except:\n",
    "            word2vec_model.wv[v] = np.zeros(200)\n",
    "    return word2vec_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGegYBe12FAR"
   },
   "outputs": [],
   "source": [
    "def get_vocab(split_sentences):\n",
    "    arr = []\n",
    "    for sentence in split_sentences:\n",
    "        for token in sentence:\n",
    "            if token not in arr:\n",
    "                arr.append(token)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLbGUrQi2FAR"
   },
   "source": [
    "TF-IDF approach to weighting the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxkKsNno2FAS"
   },
   "outputs": [],
   "source": [
    "# TF-IDF Manual Approach For Attention Vector\n",
    "# gets the overall counts of all the documents in the corpus\n",
    "def word_count_dict(vocab, data):\n",
    "    count_dict = {}\n",
    "    for v in vocab:\n",
    "        count_dict[v] = 0\n",
    "    index = 0\n",
    "    for X in data:\n",
    "        X = np.array(X)\n",
    "        if X.size <= 1:\n",
    "            continue\n",
    "        for arr in X:\n",
    "            arr = np.array(arr)\n",
    "            for token in arr:\n",
    "                try:\n",
    "                    count_dict[token] = count_dict[token] + 1\n",
    "                except:\n",
    "                    continue\n",
    "        index += 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3lHDrVq2FAS"
   },
   "outputs": [],
   "source": [
    "# gets the frequency of all terms in the selected paper\n",
    "def term_frequency(counter, data, index):\n",
    "    term_dict = {}\n",
    "    total_count = 0\n",
    "    if data[index] == None:\n",
    "        return term_dict\n",
    "    for arr in data[index]:\n",
    "        arr = np.array(arr)\n",
    "        for token in arr:\n",
    "            total_count = total_count + 1\n",
    "            \n",
    "    for c in counter:\n",
    "        term_dict[c] = counter[c] / total_count\n",
    "    \n",
    "    return term_dict, total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kiw9guAA2FAS"
   },
   "outputs": [],
   "source": [
    "# gets the log inverse document appearance of a all tokens in the document\n",
    "def inverse_term_frequency(counter, data):\n",
    "    inverse_dict = {}\n",
    "    for c in counter.keys():\n",
    "        inDoc = 0\n",
    "        for doc in data:\n",
    "            if doc == None:\n",
    "                continue\n",
    "            if any(c in x for x in np.array(doc)):\n",
    "                inDoc += 1\n",
    "            # to smooth the data, if it does not occur, say it occurred once\n",
    "        if inDoc == 0:\n",
    "            inverse_dict[c] == math.log(REFINED_CORPUS_SIZE/1)\n",
    "        else:\n",
    "            inverse_dict[c] = math.log(REFINED_CORPUS_SIZE / inDoc)\n",
    "\n",
    "    return inverse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOGUkecx2FAT"
   },
   "outputs": [],
   "source": [
    "# gets the overall tf_idf weights for the words in the vocabulary\n",
    "def tf_idf(vocab, data, index):\n",
    "    tf_idf = {}\n",
    "    counter = word_count_dict(vocab, data)\n",
    "    term_freq = term_frequency(counter, data, index)\n",
    "    inverse_term_freq = inverse_term_frequency(counter, data)\n",
    "    for term in vocab:\n",
    "        tf_idf[term] = term_freq[term] * inverse_term_freq[term]\n",
    "    return td_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mzf6Z2_H2FAT"
   },
   "outputs": [],
   "source": [
    "# MergeSort for getting top 10,000 words, as I was getting a truth value error that I could not find a fix for in the built-in functions\n",
    "def merge(l, r):\n",
    "    n = len(l) + len(r)\n",
    "    A = l\n",
    "    for key, value in r.items():\n",
    "        A[key] = value \n",
    "    keys_A = list(A.keys())\n",
    "    keys_r = list(r.keys())\n",
    "    keys_l = list(l.keys())\n",
    "    j = 0\n",
    "    k = 0\n",
    "    for i in range(0, n):\n",
    "        if (j > len(l)):\n",
    "            keys_A[i] = keys_r[k]\n",
    "            A[keys_A[i]] = r[keys_r[k]]\n",
    "            k += 1\n",
    "        elif (k > len(l)):\n",
    "            keys_A[i] = keys_l[j]\n",
    "            A[keys_A[i]] = l[keys_l[j]]\n",
    "            j += 1\n",
    "        elif (l[keys_l[j]] <= r[keys_r[k]]):\n",
    "            keys_A[i] = keys_l[j]\n",
    "            A[keys_A[i]] = l[keys_l[j]]\n",
    "            j += 1\n",
    "        else:\n",
    "            keys_A[i] = keys_r[k]\n",
    "            A[keys_A[i]] = r[keys_r[k]]\n",
    "            k += 1\n",
    "    return A\n",
    "            \n",
    "# trims the vocab down to the top ten thousand words\n",
    "# uses a MergeSort Algorithm\n",
    "def trimVocab(counter):\n",
    "    if (len(counter) == 1):\n",
    "        return counter\n",
    "    right_side = dict(list(counter.items())[len(counter)//2:])\n",
    "    left_side = dict(list(counter.items())[:len(counter)//2])\n",
    "    left_side = trimVocab(left_side)\n",
    "    right_side = trimVocab(right_side)\n",
    "    \n",
    "    counter = merge(left_side, right_side)\n",
    "    ten_thousand_most_common = dict(list(counter.items())[:10000])\n",
    "    return ten_thousand_most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQoT-pUJ2FAT"
   },
   "outputs": [],
   "source": [
    "# the counter dictionary (not a Counter object, wasn't working as well in other functions)\n",
    "counter = word_count_dict(vocab,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzusmbqA2FAT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# alternative method that I realized worked after implementing MergeSort\n",
    "# quicker than my implementation, so I switched it over\n",
    "res = dict(list(sorted(counter.items(), key = lambda x: x[1], reverse = True))[:1000])\n",
    "res_k = list(res.keys())\n",
    "trimmed_vocab = list(res.keys())\n",
    "trimmed_vocab.append(\"UNK\")\n",
    "trimmed_vocab = np.array(trimmed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETqBf7T32FAU"
   },
   "outputs": [],
   "source": [
    "# counts the number of words in a given Document\n",
    "def docCount(data, index):\n",
    "    total_count = 0\n",
    "    if data[index] == None:\n",
    "        return 0\n",
    "    for arr in data[index]:\n",
    "        arr = np.array(arr)\n",
    "        for token in arr:\n",
    "            total_count += 1\n",
    "    \n",
    "    return total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPW90f4p2FAU"
   },
   "outputs": [],
   "source": [
    "def getUniqueWords(sentences):\n",
    "    doc_vocab = []\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in doc_vocab:\n",
    "                doc_vocab.append(word)\n",
    "    return doc_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJJcycwa2FAU"
   },
   "outputs": [],
   "source": [
    "#print(docCount(X_train, 0))\n",
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiUUE6zS2FAU"
   },
   "outputs": [],
   "source": [
    "print(docCount(X_train, 0))\n",
    "print(docCount(X_train, 1))\n",
    "print(docCount(X_train, 2))\n",
    "print(docCount(X_train, 3))\n",
    "print(docCount(X_train, 4))\n",
    "print(docCount(X_train, 55))\n",
    "# 12750\n",
    "# 8064\n",
    "# 4110\n",
    "# 4428\n",
    "# 9270\n",
    "\n",
    "# 3666\n",
    "# 476\n",
    "# 0\n",
    "# 850\n",
    "# 1440\n",
    "\n",
    "# 3080\n",
    "# 1216\n",
    "# 1694\n",
    "# 4736\n",
    "# 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EN0aKKQK2FAV"
   },
   "outputs": [],
   "source": [
    "print(len(X_train[0]))\n",
    "print(len(X_train[1]))\n",
    "print(len(X_train[2]))\n",
    "print(len(X_train[3]))\n",
    "print(len(X_train[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdcvNYHN2FAV"
   },
   "outputs": [],
   "source": [
    "# maps the word to index and vice versa, for converting words to a numerical categorical value\n",
    "def mapWordToIndex(vocab):\n",
    "    w_t_i = {}\n",
    "    i_t_w = {}\n",
    "    w_t_i[\"UNK\"] = 0\n",
    "    i_t_w[0] = \"UNK\"\n",
    "    index = 1\n",
    "    for v in vocab:\n",
    "        w_t_i[v] = index\n",
    "        i_t_w[index] = v\n",
    "        index += 1\n",
    "        \n",
    "    return w_t_i, i_t_w\n",
    "\n",
    "# maps the word and indices from mapWordToIndex to the Word2Vec Embeddings\n",
    "def mapToEmbedding(i_t_w, word2vec, vocab_size):\n",
    "    i_t_e = {}\n",
    "    w_t_e = {}\n",
    "    for i, w in i_t_w.items():\n",
    "        if w == \"UNK\":\n",
    "            i_t_e[i] = np.zeros(200)\n",
    "            w_t_e[w] = np.zeros(200)\n",
    "        else:\n",
    "            i_t_e[i] = word2vec.wv[w]\n",
    "            w_t_e[w] = word2vec.wv[w]\n",
    "    return i_t_e, w_t_e\n",
    "\n",
    "# creates an embedding matrix of size (vocab_size, embedding_size)\n",
    "# needed to put in the embedding_intializer parameter in the keras Embedding Layer for the RNN\n",
    "def createEmbeddingMatrix(vocab_size, embedding_size, i_t_e):\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, embedding_size))\n",
    "    for i, e in i_t_e.items():\n",
    "        embedding_matrix[i] = e\n",
    "    return embedding_matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G90dCmK32FAW"
   },
   "outputs": [],
   "source": [
    "# splits all the sentences into a simple 2d array to process in word2vec\n",
    "split_sentences = np.array([sentence.split(\" \") for sentence in sentence_dump])\n",
    "#word2vec_model = Word2Vec(sentences = X_train[0], sg = 1, window = 5, size = 200, min_count = 1)\n",
    "#word2vec_model = fill_in_blanks(vocab, word2vec_model)\n",
    "#ype(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBKrGRxN2FAW"
   },
   "outputs": [],
   "source": [
    "# Connect Documents into a single string representation, sentences seperated by \"\\n\"\n",
    "def connect_document(sentences):\n",
    "    document = \"\"\n",
    "    for sentence in sentences:\n",
    "        sent = sentence + \"\\n\"\n",
    "        document = document + sent\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgCE-1Ce2FAW"
   },
   "outputs": [],
   "source": [
    "def numWords(article):\n",
    "    words = 0\n",
    "    for sentence in article:\n",
    "        words = words + len(sentence)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "707CD7Xw2FAX"
   },
   "source": [
    "##### RNN\n",
    "### Pointer Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biqjMqTd2FAX"
   },
   "outputs": [],
   "source": [
    "def truncate(word_limit, data):\n",
    "    word_counter = 0\n",
    "    truncated = list()\n",
    "    for sentence in data:\n",
    "        if word_counter >= word_limit:\n",
    "            break\n",
    "        sent = list()\n",
    "        for word in sentence:\n",
    "            if word_counter == word_limit:\n",
    "                break\n",
    "            else:\n",
    "                word_counter = word_counter + 1\n",
    "                sent.append(word)\n",
    "        truncated.append(sent)\n",
    "    return truncated     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grTLlVzR2FAY"
   },
   "outputs": [],
   "source": [
    "def pointer_generator(train_model_indices, attention_dist, seq_len = 4000, vocab_size = 2500, embedding_size = 100):\n",
    "    # Initial Set Up\n",
    "    list_of_docs = []\n",
    "    for i in train_model_indices:\n",
    "        list_of_docs.append(truncate(seq_len, X_train[i]))\n",
    "    initial_data = combine(list_of_docs)\n",
    "    encoder = Model()\n",
    "    # Encoder\n",
    "    # Embedding: Tensor of shape [batch_size, encoder_steps, embedding_size]\n",
    "    # encoder steps is number of separate articles included\n",
    "    token_embedding = Embedding(len(vocab),len(train_model_indices),embedding_size)\n",
    "    token_embeddings = token_embedding(tf.keras.Input(shape=(None,), dtype = 'int32'))\n",
    "    # LSTM: Shape[batch_size, hidden_dim] and Bi-Directional and reduce using merge_mode\n",
    "    lstm = Bidirectional(LSTM(round(embedding_size / 2), return_sequences=True, return_state=True), input_shape=(1, 50), merge_mode=\"mul\")\n",
    "    lstm_func = lstm(token_embeddings)\n",
    "    \n",
    "    # Add Attention Decoder\n",
    "    decoder = Model()\n",
    "    dec_embedding = Embedding(vocab_size, embedding_size)\n",
    "    \n",
    "    attention_mechanism = tfa.seq2seqLuongAttention(units=seq_len, memory = [round(embedding_size/2), ], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ew5-V8rs2FAY"
   },
   "outputs": [],
   "source": [
    "# helper function for RNN, where most of the action happens\n",
    "# for each individual document\n",
    "def add_new_word(word_data, cur_sum, inputs_src, pos):\n",
    "    vocab_size = trimmed_vocab.size + 1\n",
    "    # Average abstract length is 150-250 words in length, so I thought 200 words would be a good length.\n",
    "    sum_txt_length = pos\n",
    "    # source side for Hidden Layer W\n",
    "    \n",
    "    # overloaded my application memory even with one epoch\n",
    "    src_embedding = Embedding(vocab_size, 200, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), trainable=False)(inputs_src)\n",
    "    src_hidden_layer = LSTM(200)(src_embedding)\n",
    "    #sum side for Hidden Layer U\n",
    "    \n",
    "    # did not use pre-trained word embeddings as this is supposed to take into account the already used words in the summary \n",
    "    inputs_cur_sum = Input(shape=(sum_txt_length,))\n",
    "    cur_sum_embedding = Embedding(vocab_size, 200)(inputs_cur_sum)\n",
    "    cur_sum_hidden_layer = LSTM(200)(cur_sum_embedding)\n",
    "    #decoder side for Hidden Layer V\n",
    "    attention_result = Attention()([src_hidden_layer, cur_sum_hidden_layer])\n",
    "    decoder = tf.concat([attention_result, cur_sum_hidden_layer], 1)\n",
    "    decoded = Dense(vocab_size, activation='softmax')(decoder)\n",
    "    \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMWurF3o2FAZ"
   },
   "outputs": [],
   "source": [
    "# use word2vec as a model input\n",
    "#w_t_i, i_t_w = mapWordToIndex(trimmed_vocab)\n",
    "#i_t_e, w_t_e = mapToEmbedding(i_t_w, word2vec_model, trimmed_vocab.size)\n",
    "#embedding_matrix = createEmbeddingMatrix(trimmed_vocab.size, 200, i_t_e)\n",
    "#word2vec_model = Word2Vec(sentences = sentences_dump, sg = 1, window = 5, size = 200, min_count = 1)\n",
    "#embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7c8cgGi2FAZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix_train = create_embedding_matrix(X_train[0])\n",
    "print(type(embedding_matrix_train))\n",
    "embedding_matrix_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4rlt3wE2FAZ"
   },
   "source": [
    "### Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NNs0Mc42FAa"
   },
   "outputs": [],
   "source": [
    "# RNN function\n",
    "def create_and_compile_encoder(word_data):\n",
    "    # Average abstract length is 150-250 words in length, so I thought 200 words would be a good length.\n",
    "    sum_txt_length = 200\n",
    "    # Encoder\n",
    "    embedding_matrix = create_embedding_matrix(X_train[0])\n",
    "    model = Sequential()\n",
    "    #embedding_matrix = create_embedding_matrix(word_data)\n",
    "    \n",
    "    input_size = len(getUniqueWords(word_data))\n",
    "    layer_size = round(input_size/5)\n",
    "    model.add(Embedding(input_dim = input_size, output_dim = 50, \n",
    "                        weights = [embedding_matrix], trainable = True, mask_zero = True))\n",
    "    model.add(Masking())\n",
    "    while (layer_size > 200):\n",
    "        if (layer_size <= 1000):\n",
    "            model.add(LSTM(200, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "            layer_size = 200\n",
    "        else:\n",
    "            model.add(LSTM(round(layer_size/5), return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
    "            layer_size = round(layer_size/5)\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dense(input_size, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss = 'categorical_crossentropy')\n",
    "    model.summary()\n",
    "    print(\"Model Compiled\")\n",
    "    return model\n",
    "    #attention layer\n",
    "    # the distribution is the TF-IDF for this document\n",
    "#     attention_dist = tf_idf(trimmed_vocab, X_train, 0)\n",
    "#    attention_result = Attention()([decoded, ])\n",
    "#     simple_model = Model(inputs=inputs_src, outputs = decoded)\n",
    "\n",
    "#     simple_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckpEvWH72FAa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_and_compile_encoder(X_train[0])\n",
    "model_X = one_hot_y(getUniqueWords(X_train[0]), X_train[0])\n",
    "model_y = one_hot_y(getUniqueWords(X_train[0]), y_train[0])\n",
    "\n",
    "history = model.fit(model_X.T, \n",
    "                    model_y.T, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9E6r-jc2FAb"
   },
   "outputs": [],
   "source": [
    "print(type(X_train[0]))\n",
    "print(type(X_train[0][0]))\n",
    "np.array(X_train[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzC_Zdis2FAb"
   },
   "outputs": [],
   "source": [
    "y_train[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDShiaHv2FAc"
   },
   "source": [
    "### Attention Based Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-4w5M8g2FAc"
   },
   "outputs": [],
   "source": [
    "def attention_encode(word_data, context_embedding):\n",
    "    # initial context is random\n",
    "    # enc = p^T (x_mean)\n",
    "    # p is exp(x_approx P y_approx)\n",
    "    # x_approx = [Fx_1 ... Fx_m]\n",
    "    # y_approx = [Gy_(i - C+1), ... Gy_i]\n",
    "    # For all x in \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xvC1UM52FAc"
   },
   "source": [
    "### Function that creates the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1_51omw2FAd"
   },
   "outputs": [],
   "source": [
    "# creaes the RNN, compiles it, and returns the model\n",
    "def document_summarize(sum_length, article_choice, simple):\n",
    "    model = Model()\n",
    "    # shape is number of words in the article\n",
    "    inputs_src = Input(shape=(numWords(article_choice),))\n",
    "    cur_sum = Input(shape=(None,))\n",
    "    output_sum = cur_sum\n",
    "    if simple:\n",
    "        output_sum = tf.concat([output_sum,add_new_word_simple(article_choice, output_sum, inputs_src, i)], axis = 1)\n",
    "    else:\n",
    "        output_sum = add_new_word(article_choice, output_sum, inputs_src, i)\n",
    "    if simple:\n",
    "        print(inputs_src.shape)\n",
    "        print(output_sum)\n",
    "        model = Model(inputs=inputs_src, outputs = output_sum)\n",
    "    else:\n",
    "        sum_len = Input(shape=(sum_length,))\n",
    "        model = Model(inputs=[inputs_src,cur_sum], outputs = output_sum)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p57QTcre2FAd"
   },
   "source": [
    "### The next cell throws an error as the models do not compile properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-h7kMdF2FAd"
   },
   "source": [
    "### Generates the batch data for the step in the epoch of training the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMsogt-H2FAd"
   },
   "outputs": [],
   "source": [
    "def generateStepper(X_train, y_train, vocab, simple):\n",
    "    index = 0\n",
    "    # will pass the data as indices\n",
    "    w_t_i, i_t_w = mapWordToIndex(vocab)\n",
    "    while True:\n",
    "        while (X_train[index] == None) or (y_train[index] == None):\n",
    "            index+=1\n",
    "        X_start = X_train[index]\n",
    "        X_batch = []\n",
    "        src_txt_length = docCount(X_train, index)\n",
    "        req_length = 0\n",
    "        # creates the X_batch data\n",
    "        for sents in X_start:\n",
    "            sents = list(sents)\n",
    "            if req_length >= 7000:\n",
    "                break\n",
    "            for token in sents:\n",
    "                if req_length >= 7000:\n",
    "                    break\n",
    "                req_length += 1\n",
    "                try:\n",
    "                    new_input = w_t_i[token]\n",
    "                except:\n",
    "                    new_input = w_t_i[\"UNK\"]\n",
    "                X_batch.append(new_input)\n",
    "        while req_length < 7000:\n",
    "            req_length += 1\n",
    "            X_batch.append(0)\n",
    "        # creates the y_true value\n",
    "        y_src = y_train[index]\n",
    "        y_batch = []\n",
    "        index = 0\n",
    "        for y_sent in y_src:\n",
    "            y_sent = list(y_sent)\n",
    "            if index >= 20:\n",
    "                break\n",
    "            for token in y_sent:\n",
    "                if index >= 20:\n",
    "                    break\n",
    "                index += 1\n",
    "                try:\n",
    "                    y_batch.append(w_t_i[token])\n",
    "                except:\n",
    "                    y_batch.append(0)\n",
    "        index += 1\n",
    "        # as of now: the output is also the input to the pointer generator\n",
    "        # I know that is wrong. It should be an array that start with a special initializer token \n",
    "        # and after each step it should add the newly generated value\n",
    "        if simple:\n",
    "            yield np.array(X_batch), np.array(y_batch)\n",
    "        else:\n",
    "            yield [np.array(X_batch), np.array(y_batch)], np.array(y_batch)\n",
    "        X_start = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snFp89pT2FAe"
   },
   "outputs": [],
   "source": [
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Io-yL-K52FAe"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3m758dOv2FAe"
   },
   "outputs": [],
   "source": [
    "# GOAL TF-IDF to re-weight the embeddings between steps/epochs\n",
    "document_data_generator = generateStepper(X_train, y_train, trimmed_vocab, True)\n",
    "simple_model.fit(document_data_generator, steps_per_epoch = 1, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FimeWqG2FAf"
   },
   "outputs": [],
   "source": [
    "# Needed Steps (Unfinished)\n",
    "# Actually translating to words\n",
    "#l, y = next(document_data_generator)\n",
    "#model.predict(l[0])\n",
    "# post-processing step\n",
    "# combination - get the top 200 from both and find where they overlap and use overlap. Might need to find larger sets\n",
    "#2. linear scaling - multiply together after smoothing tfidfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHe7Owzf2FAf"
   },
   "source": [
    "Sources:\n",
    "1. https://machinelearningmastery.com/gentle-introduction-text-summarization/\n",
    "2. https://stackoverflow.com/questions/28373282/how-to-read-a-json-dictionary-type-file-with-pandas\n",
    "3. https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85\n",
    "4. http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html\n",
    "5. https://www.scribbr.com/apa-style/apa-abstract/\n",
    "6. https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tanner Lederman Final Project Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
